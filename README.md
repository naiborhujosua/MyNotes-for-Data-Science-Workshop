# Mynotes-for-Data-Science-Workshop
[![versions](https://img.shields.io/pypi/pyversions/pybadges.svg)](https://www.python.org/downloads/)

Being a self-taught **data scientist** is quite challenging for me .There are many factors that i have to face in order to advance this field. That's why i would like to take notes for every online courses that i will take in many platforms such as Coursera, Packt, Udacity and Datacamp. I hope i can reinforce my learning rate into the right skills that could be implemented in my new career as a Data Scientist. Today's workshop is taken from one of the workshop from Packt called [Applied Data Science Workshop](https://courses.packtpub.com/collections?page=3). I hope we can learn together in order to dive deep into this field. 

 As a Data Scientist, before exploring the data, we have to prepare the tools that we will be using to get the insights of our data. We will be installing python or R. The easiet way to install it by downloading [Anaconda](https://www.anaconda.com/). After installing the tools, we can install some libraries that will help us do the analysis of our data. in this case, we will use *pip* as package manager which is included in Anaconda through these command
- pip install numpy
+ pip install -r requirement.txt

The exercises and activities will be excuted in Jupyter Notebooks. It is a python library that can be installed using pip install jupyter. Since it is included in our Anaconda, we do not need it. we can run jupyter notebook by running a command jupyter notebook through our terminal in MaC or command prompt in windows. 

## Download the Course in Github
- Clone [Applied Data Science Workshop](https://github.com/PacktWorkshops/The-Applied-Data-Science-Workshop)
+ cd The-Applied-Data-Science-Workshop in your terminal to access the data
- run jupyter notebook to access the data in this workshop

Tips for using jupyter notebook to accelerate workflow 
* [x] Esc + m to change the cell into markdown
* [x] a to add a cell above
* [x] b to add a cell below
* [x] dd to delete a cell
* [x] esc + y to change a cell to code


## Data Science Lifecycle :
![Data Science Lifecycle](https://github.com/naiborhujosua/Notes-for-Data-Science-Workshop/blob/master/data%20science%20process.jpg)

<br><br>
Data Exploration with Jupyter
---

1. [Boston Housing Dataset Analysis](https://github.com/naiborhujosua/MyNotes-for-Data-Science-Workshop/blob/master/Boston%20Housing%20Dataset.ipynb) I have been  learning about exploratory analysis in a jupyter notebook environment. I used vizualization such as scatter plots, histograms, and violin plots to deepen my understanding of the data, I also performed simple predictive modelling to consider the data for modelling. you can download the notebook to see the process. As I continue to learn and understand more, I will update this workshop for deeper contents.
2. [Training Classification Models](https://github.com/naiborhujosua/MyNotes-for-Data-Science-Workshop/blob/master/Training%20Model%20Classifiers.ipynb) I have been  learning about cleaning the data before implmenting three essentials Classification algorithms such as Support Vector Machine, KNN, and Random Forest. i also implemented how to quantify the best score best on confusion matrix to ensure the best accuracy on these algorihms. It is also important to split the dataset and scale the features before implementing the classification models to get better insights. You can also see the exercise on implementing [SVMs algorithm](https://github.com/naiborhujosua/MyNotes-for-Data-Science-Workshop/blob/master/Implementing%20SVM%20algorithms.ipynb) 
2. [Model Validation and Optimization](https://github.com/naiborhujosua/MyNotes-for-Data-Science-Workshop/blob/master/Model%20Validation%20and%20Optimization.ipynb) I have been  learning about performing parameter optimization and model selection. We built upon the work we did where we trainer predictive classification models for our bnary problems and saw how decision boundaries are drawn for SVM, KNN, and Random Forest models. We improved these simple models by using validation curve to optimize parameters and explored how dimensionality reduction can improve model performcane as well. 




